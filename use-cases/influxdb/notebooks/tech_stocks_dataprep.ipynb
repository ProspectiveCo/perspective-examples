{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech Stocks Data Prep\n",
    "\n",
    "Code to prepare portions of the data stocks data for the influxdb demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, date\n",
    "from typing import Iterable, List\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Subset\n",
    "\n",
    "Take only 2017-2018 stocks trade data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading large source by chunks..............................................\n",
      "total records: 4,449,431 - filtered records: 53,890\n",
      "concatenating 2 dataframes...\n",
      "shape: (53890, 13)\n",
      "date min: 2018-08-01 00:00:00 - max: 2018-08-24 00:00:00\n",
      "sorting and post-processing....\n",
      "writing to output file: ../influxdb-docker/volumes/data/trades-md.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# configuration vars\n",
    "SOURCE_LARGE_STOCKS_DATAFILE = r\"../../../data-generation/stocks/data/tech_trades.csv\"\n",
    "OUTPUT_FILTERED_STOCK_DATAFILE = r\"../influxdb-docker/volumes/data/trades-md.tar.gz\"\n",
    "FILTER_MIN_DATE: date = datetime(2018, 8, 1)\n",
    "FILTER_MAX_DATE: date = datetime(2018, 9, 1)\n",
    "FILTER_DATE_RANGE = (FILTER_MIN_DATE, FILTER_MAX_DATE)\n",
    "OFFSET_TIMESTAMPS = True        # to offset the timestamps to start from now and go backwards\n",
    "\n",
    "\n",
    "# check to see if input file exists\n",
    "assert os.path.exists(SOURCE_LARGE_STOCKS_DATAFILE), f\"Original stocks data file does NOT exist: ${SOURCE_LARGE_STOCKS_DATAFILE}\"\n",
    "# create the output folder if it doesn't exists\n",
    "if not os.path.isdir(os.path.dirname(OUTPUT_FILTERED_STOCK_DATAFILE)):\n",
    "    print(f\"Creating output folder: ${os.path.dirname(OUTPUT_FILTERED_STOCK_DATAFILE)}\")\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILTERED_STOCK_DATAFILE), mode=0o740, exist_ok=True)\n",
    "\n",
    "# read source csv in chunks\n",
    "chunks: Iterable[pd.DataFrame] = pd.read_csv(SOURCE_LARGE_STOCKS_DATAFILE, chunksize=100_000, on_bad_lines='skip')\n",
    "filtered_dfs: List[pd.DataFrame] = []   # buffer to hold filtered df chunks\n",
    "total_records = 0\n",
    "filtered_records = 0\n",
    "print(f\"Reading large source by chunks.\", end='')\n",
    "for df in chunks:\n",
    "    # convert datetime cols\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce', format='%Y-%m-%d')\n",
    "    # add up total records processed\n",
    "    total_records += len(df.index)\n",
    "    # filter by date range\n",
    "    # df = df[df['date'].dt.year.isin(FILTERED_YEARS)]\n",
    "    df = df[ (df['date'] >= FILTER_MIN_DATE) & (df['date'] < FILTER_MAX_DATE) ]\n",
    "    if len(df.index):\n",
    "        filtered_records += len(df.index)\n",
    "        filtered_dfs.append(df.copy())\n",
    "    # print a trailing process dot\n",
    "    print(\".\", end='')\n",
    "# process done. print newline & progress report\n",
    "print(f\"\\ntotal records: {total_records:,} - filtered records: {filtered_records:,}\")\n",
    "# concatenate dataframes\n",
    "print(f\"concatenating {len(filtered_dfs)} dataframes...\")\n",
    "df = pd.concat(filtered_dfs, ignore_index=True)\n",
    "# display resulting dataframe info\n",
    "print(f\"shape: {df.shape}\")\n",
    "print(f\"date min: {df['date'].min()} - max: {df['date'].max()}\")\n",
    "# post-processing\n",
    "print(f\"sorting and post-processing....\")\n",
    "df.sort_values(by=['trade_timestamp', 'ticker'], ignore_index=True, inplace=True)\n",
    "# dropping cols\n",
    "df.drop(columns=['trade_id'], inplace=True, errors='ignore')\n",
    "# convert the timestamp to unix nano seconds\n",
    "df['trade_timestamp'] = pd.to_datetime(df['trade_timestamp'], errors='coerce', format=r'%Y-%m-%d %H:%M:%S')\n",
    "df['trade_timestamp'] = df['trade_timestamp'].astype('int64')\n",
    "# offset the records timestamps to start from now() and go backwards in time\n",
    "if OFFSET_TIMESTAMPS:\n",
    "    # calculate the offset between the largest timestamp and now\n",
    "    offset = int(datetime.now().timestamp() * 1_000_000_000) - df['trade_timestamp'].max()\n",
    "    # shift all timestamps up\n",
    "    df['trade_timestamp'] += offset\n",
    "# write output file\n",
    "print(f\"writing to output file: {OUTPUT_FILTERED_STOCK_DATAFILE}\")\n",
    "df.to_csv(OUTPUT_FILTERED_STOCK_DATAFILE, mode='w', index=False, compression='infer', quoting=csv.QUOTE_MINIMAL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
