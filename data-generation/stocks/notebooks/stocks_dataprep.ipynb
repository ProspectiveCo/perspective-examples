{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Trading Data Preparation\n",
    "\n",
    "Prepare historical stock prices datasets for demo analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Source\n",
    "\n",
    "The original data source if on Kaggle: [**https://www.kaggle.com/datasets/ehallmar/daily-historical-stock-prices-1970-2018**](https://www.kaggle.com/datasets/ehallmar/daily-historical-stock-prices-1970-2018)\n",
    "\n",
    "- This dataset is cleaned up to only include major tech companies (from the list below)\n",
    "- It is also truncated to only include 2012-2018 data\n",
    "\n",
    "The resulting file is stored in: [`data/historical_tech_stock_prices.csv`](../../data/historical_tech_stock_prices.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Original Dataset\n",
    "\n",
    "Download and unzip the original dataset (1.8GB) from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from kaggle: https://www.kaggle.com/api/v1/datasets/download/ehallmar/daily-historical-stock-prices-1970-2018\n",
      "This will take few minutes...\n",
      "Writing to file: ../../data/historical_stocks.zip\n",
      "Download completed successfully.\n",
      "Extracted historical_stock_prices.csv to ../../data\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# URL of the file to be downloaded\n",
    "url = \"https://www.kaggle.com/api/v1/datasets/download/ehallmar/daily-historical-stock-prices-1970-2018\"\n",
    "\n",
    "# Destination path for the downloaded file\n",
    "data_dir = r\"../../data\"\n",
    "zip_file = os.path.join(data_dir, \"historical_stocks.zip\")\n",
    "data_file = \"historical_stock_prices.csv\"  # The specific data file to extract\n",
    "\n",
    "# Step 1: Download the ZIP file\n",
    "# Download the file with streaming enabled\n",
    "print(f\"Downloading from kaggle: {url}\\nThis will take few minutes...\")\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Write the file content to the destination file in chunks\n",
    "    with open(zip_file, \"wb\") as file:\n",
    "        print(f\"Writing to file: {zip_file}\")\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:  # Ensure the chunk is not empty\n",
    "                file.write(chunk)\n",
    "    print(\"Download completed successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download. Status code: {response.status_code}\")\n",
    "    raise RuntimeError()\n",
    "\n",
    "# Step 2: Extract only the target file from the ZIP archive\n",
    "try:\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        # Check if the target file exists in the archive\n",
    "        if data_file in zip_ref.namelist():\n",
    "            print(f\"Extracting zip file: {data_file}\")\n",
    "            zip_ref.extract(data_file, data_dir)\n",
    "            print(f\"Extracted {data_file} to {data_dir}\")\n",
    "        else:\n",
    "            print(f\"Error: {data_file} not found in the archive.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(\"Error: The downloaded file is not a valid ZIP archive.\")\n",
    "\n",
    "# step 3: clean up\n",
    "print(f\"Removing downloaded zip file: {zip_file}\")\n",
    "os.remove(zip_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up Original Dataset\n",
    "\n",
    "Clean up the data to only include:\n",
    "- Tech stocks from the list below\n",
    "- Stocks from 2012-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# List of top performing tech companies stock symbols\n",
    "TECH_SYMBOLS = [\n",
    "    \"AAPL\",   # Apple Inc.\n",
    "    \"MSFT\",   # Microsoft Corporation\n",
    "    \"GOOGL\",  # Alphabet Inc. (Class A)\n",
    "    \"GOOG\",   # Alphabet Inc. (Class C)\n",
    "    \"AMZN\",   # Amazon.com Inc.\n",
    "    \"FB\",     # Meta Platforms, Inc. (formerly Facebook)\n",
    "    \"NFLX\",   # Netflix, Inc.\n",
    "    \"NVDA\",   # NVIDIA Corporation\n",
    "    \"TSLA\",   # Tesla, Inc.\n",
    "    \"INTC\",   # Intel Corporation\n",
    "    \"CSCO\",   # Cisco Systems, Inc.\n",
    "    \"ADBE\",   # Adobe Inc.\n",
    "    \"ORCL\",   # Oracle Corporation\n",
    "    \"IBM\",    # International Business Machines Corporation\n",
    "    \"CRM\",    # Salesforce, Inc.\n",
    "    \"PYPL\",   # PayPal Holdings, Inc.\n",
    "    \"AMD\",    # Advanced Micro Devices, Inc.\n",
    "    \"TXN\",    # Texas Instruments Incorporated\n",
    "    \"QCOM\",   # Qualcomm Incorporated\n",
    "    \"AVGO\",   # Broadcom Inc.\n",
    "    \"SHOP\",   # Shopify Inc.\n",
    "    \"SNAP\",   # Snap Inc.\n",
    "    \"TWTR\",   # Twitter, Inc.\n",
    "    \"SQ\",     # Block, Inc. (formerly Square)\n",
    "    \"DOCU\"    # DocuSign, Inc.\n",
    "]\n",
    "\n",
    "data_dir = r\"../../data\"\n",
    "data_file = os.path.join(data_dir, \"historical_stock_prices.csv\")\n",
    "output_file = os.path.join(data_dir, \"historical_tech_stock_prices.csv\")\n",
    "\n",
    "# write the headers\n",
    "with open(output_file, mode='w') as outfile:\n",
    "    outfile.write(\"ticker,open,close,adj_close,low,high,volume,date\\n\")\n",
    "# load csv\n",
    "chunks = pd.read_csv(data_file, chunksize=10000)\n",
    "for df in chunks:\n",
    "    df = df[(df['ticker'].isin(TECH_SYMBOLS)) & (df['date'] >= '2012-01-01')]\n",
    "    # sort and output\n",
    "    df.to_csv(output_file, mode='a', index=False, header=False)\n",
    "\n",
    "# read back and sort by date and ticker\n",
    "df = pd.read_csv(output_file)\n",
    "df.sort_values(by=['date', 'ticker'], ignore_index=True, inplace=True)\n",
    "df.to_csv(output_file, index=False, header=True)\n",
    "\n",
    "# print min/max reported stock dates per ticker\n",
    "df[['ticker', 'date']].groupby(by='ticker').agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Daily Stock Trades \n",
    "\n",
    "Based on the cleaned up tech historical prices (above), generate a series of daily trades performed by different brokers.\n",
    "\n",
    "- Stocks are traded in **logarithmic** daily distribution where some stocks are traded at higher quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate brokers\n",
    "broker_names = [\n",
    "    \"Slick Sam\", \"Trading Tina\", \"Money Mike\", \"Clever Cathy\", \"Profit Pete\", \n",
    "    \"Risky Rachel\", \"Big Bucks Bob\", \"Smart Susan\", \"Lucky Luke\"\n",
    "]\n",
    "brokers = broker_names\n",
    "\n",
    "# Parameters\n",
    "min_trades_per_day, max_trades_per_day = 50, 200\n",
    "share_prct_range = (0.00001, 0.0001)\n",
    "\n",
    "# Read historical stock price data\n",
    "data = pd.read_csv(\"data/historical_tech_stock_prices.csv\", parse_dates=['date'])\n",
    "\n",
    "# Create a list to hold generated trades\n",
    "trades = []\n",
    "\n",
    "# Get min and max dates from the data\n",
    "min_date = data['date'].min()\n",
    "max_date = data['date'].max()\n",
    "# max_date = data['date'].min() + timedelta(days=5)\n",
    "print(f\"generating between: {min_date} - {max_date}\")\n",
    "\n",
    "# Create a date range from min to max\n",
    "date_range = pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "# Traverse the data day by day\n",
    "for current_date in date_range:\n",
    "    # Filter data for the current day\n",
    "    day_data = data[data['date'] == current_date]\n",
    "    if day_data.empty:\n",
    "        continue\n",
    "    \n",
    "    # Sort tickers for this day and apply a smooth logarithmic curve to prioritize larger trades for certain tickers\n",
    "    num_tickers = len(day_data['ticker'].unique())\n",
    "    log_weights = np.logspace(0, -1, num=num_tickers)  # Smooth logarithmic distribution\n",
    "    day_data = day_data.assign(weight=log_weights)\n",
    "    day_data = day_data.sort_values(by='weight', ascending=False)\n",
    "    \n",
    "    # For each ticker in the current day, generate a series of trades\n",
    "    for _, row in day_data.iterrows():\n",
    "        # Get number of trades for the ticker on this day\n",
    "        num_trades = random.randint(min_trades_per_day, max_trades_per_day)\n",
    "        \n",
    "        # Generate trades for the ticker\n",
    "        for _ in range(num_trades):\n",
    "            # Random broker\n",
    "            broker = random.choice(brokers)\n",
    "            \n",
    "            # Random timestamps on the current date\n",
    "            trade_timestamp = current_date + timedelta(seconds=random.randint(0, 86399))\n",
    "            \n",
    "            # Bid and ask prices between high and low for the day\n",
    "            bid_price = np.random.uniform(row['low'], row['high'])\n",
    "            ask_price = np.random.uniform(row['low'], row['high'])\n",
    "            while ask_price <= bid_price:  # Ensure ask is higher than bid\n",
    "                ask_price = np.random.uniform(row['low'], row['high'])\n",
    "                \n",
    "            # Trade price between bid and ask\n",
    "            trade_price = np.random.uniform(bid_price, ask_price)\n",
    "            \n",
    "            # Bid-ask spread\n",
    "            bid_spread = ask_price - bid_price\n",
    "            \n",
    "            # Determine number of shares based on the logarithmic weight and the day's volume\n",
    "            shares = int(row['volume'] * row['weight'] * np.random.uniform(*share_prct_range))\n",
    "            \n",
    "            # Calculate trade value\n",
    "            trade_value = round(trade_price * shares, ndigits=6)\n",
    "            \n",
    "            # Create the trade record\n",
    "            trade = {\n",
    "                'trade_timestamp': trade_timestamp,\n",
    "                'ticker': row['ticker'],\n",
    "                'broker': broker,\n",
    "                'bid_price': round(bid_price, 4),\n",
    "                'ask_price': round(ask_price, 4),\n",
    "                'trade_price': round(trade_price, 4),\n",
    "                'bid_spread': round(bid_spread, 4),\n",
    "                'shares': shares,\n",
    "                'trade_value': round(trade_value, 4),\n",
    "                # meta columns\n",
    "                'open': round(row['open'], 6),\n",
    "                'close': round(row['adj_close'], 6),\n",
    "                'date': row['date'],\n",
    "            }\n",
    "            \n",
    "            # Include all the original stock data for the day\n",
    "            # for col in row.index:\n",
    "            #     trade[col] = row[col]\n",
    "            \n",
    "            # Append the trade record to the trades list\n",
    "            trades.append(trade)\n",
    "    # finished current date\n",
    "    print(f\"finished date: {current_date}\")\n",
    "\n",
    "# Convert the list of trades into a pandas DataFrame\n",
    "trades_df = pd.DataFrame(trades)\n",
    "# sort values\n",
    "# trades_df.sort_values(by=['trade_timestamp', 'ticker', 'broker'], ignore_index=True, inplace=True)\n",
    "\n",
    "# Output the generated trades DataFrame to a CSV file\n",
    "trades_df.to_csv(\"data/tech_trades.csv\", index=False)\n",
    "\n",
    "print(\"Trades simulation complete. Output saved to data/tech_trades.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# sort trades by timestamp and write back\n",
    "df = pd.read_csv(\"data/tech_trades.csv\")\n",
    "df.sort_values(by=['trade_timestamp', 'ticker', 'broker'], ignore_index=True, inplace=True)\n",
    "# reindex\n",
    "df.drop(columns=['trade_id'], inplace=True, errors='ignore')\n",
    "df.insert(0, 'trade_id', df.index + 100000)\n",
    "df.to_csv(\"data/tech_trades.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rewrite the entire final data file\n",
    "    - remove teh \"Broker #: \"\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "# sort trades by timestamp and write back\n",
    "df = pd.read_csv(\"data/tech_trades.csv\", nrows=10)\n",
    "df['broker'] = df['broker'].map(lambda x: str(x).split(': ')[1])\n",
    "df.to_csv(\"data/tech_trades.csv\", index=False)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# sort trades by timestamp and write back\n",
    "df = pd.read_csv(\"data/tech_trades.csv\", nrows=10000)\n",
    "schema = {\n",
    "    k: 'string' if str(v) == 'O' else \\\n",
    "       'string' if str(v) == 'object' else \\\n",
    "       'float' if str(v) == 'float64' else \\\n",
    "       'integer' if str(v) == 'int64' else \\\n",
    "       str(v)\n",
    "    for k, v in dict(df.dtypes).items()\n",
    "}\n",
    "\n",
    "print(schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
